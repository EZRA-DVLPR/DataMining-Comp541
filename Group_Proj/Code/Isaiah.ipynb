{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id              artist_name                  release_name  \\\n",
      "0    16493     Greg MacPherson Band  Good Times Coming Back Again   \n",
      "1     8793  Wolfgang Amadeus Mozart     The World of Sacred Music   \n",
      "2     6263                    Japan                      Tin Drum   \n",
      "3     5838                   Enigma          The Cross of Changes   \n",
      "4     1061               Paul Simon                     Graceland   \n",
      "\n",
      "                                      recording_name        date      time  \n",
      "0                                            Numbers  2006-11-29  13:19:10  \n",
      "1                                   Ave Verum Corpus  2006-11-29  13:52:16  \n",
      "2                                             Ghosts  2006-11-29  13:59:42  \n",
      "3                   Age of Loneliness (Carlyâ€™s Song)  2006-11-29  13:55:42  \n",
      "4  All Around the World or the Myth of Fingerprin...  2006-11-29  14:04:29   \n",
      " (66936, 6)\n"
     ]
    }
   ],
   "source": [
    "#import libraries and functions\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import dataset as a dataframe from a csv\n",
    "df = pd.read_csv('../../Project1/completeData17.csv')\n",
    "\n",
    "#print first 5 rows of df\n",
    "print(df.head(), \"\\n\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  artist_name  release_name  recording_name        date      time\n",
      "0    16493         3665          4240            9463  2006-11-29  13:19:10\n",
      "1     8793         9899         10764            1240  2006-11-29  13:52:16\n",
      "2     6263         4286         10945            5040  2006-11-29  13:59:42\n",
      "3     5838         2913          9918             676  2006-11-29  13:55:42\n",
      "4     1061         6645          4260             759  2006-11-29  14:04:29 \n",
      " (66936, 6)\n"
     ]
    }
   ],
   "source": [
    "#encode the categorical variables:\n",
    "#artist_name, release_name, recording_name\n",
    "\n",
    "df['artist_name'] = df['artist_name'].astype('category').cat.codes\n",
    "df['release_name'] = df['release_name'].astype('category').cat.codes\n",
    "df['recording_name'] = df['recording_name'].astype('category').cat.codes\n",
    "\n",
    "#print new head and shape of df\n",
    "print(df.head(), \"\\n\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop(['date', 'time'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user_id  artist_name  release_name  recording_name\n",
      "25722      366         1165          4698           12845\n",
      "63742     2891         7625           819           11501\n",
      "43332      677          599          4970           12926\n",
      "41491     9441          134           440           11053\n",
      "37914     1953         4657         11353            2414 \n",
      "        user_id  artist_name  release_name  recording_name\n",
      "8707       261         8817          8721           11601\n",
      "43439    12040         9266          2025            2530\n",
      "25257    28549         4581          6824            7951\n",
      "42095     2967         4940          9597            7280\n",
      "11149      642         7023         12248            9059\n",
      "Sizes:\t train\t\t test\n",
      "\t 53548 \t 13388\n"
     ]
    }
   ],
   "source": [
    "#train/test split\n",
    "#80/20\n",
    "train, test = train_test_split(df2, test_size = 0.2)\n",
    "\n",
    "#print head of each set followed by sizes of each set\n",
    "print(train.head(), \"\\n\", test.head())\n",
    "\n",
    "print(\"Sizes:\\t train\\t\\t test\")\n",
    "print(\"\\t\", train.shape[0], \"\\t\", test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artist_name  0      1      2      3      4      5      6      7      8      \\\n",
      "user_id                                                                      \n",
      "5                0      0      0      0      0      0      0      0      0   \n",
      "6                0      0      0      0      0      0      0      0      0   \n",
      "10               0      0      0      0      0      0      0      0      0   \n",
      "11               0      0      0      0      0      0      0      0      0   \n",
      "14               0      0      0      0      0      0      0      0      0   \n",
      "\n",
      "artist_name  9      ...  10493  10494  10495  10496  10497  10498  10499  \\\n",
      "user_id             ...                                                    \n",
      "5                0  ...      0      0      0      0      0      0      0   \n",
      "6                0  ...      0      0      0      0      0      0      0   \n",
      "10               0  ...      0      0      0      0      0      0      0   \n",
      "11               0  ...      0      0      0      0      0      0      0   \n",
      "14               0  ...      0      0      0      0      0      0      0   \n",
      "\n",
      "artist_name  10500  10501  10502  \n",
      "user_id                           \n",
      "5                0      0      0  \n",
      "6                0      0      0  \n",
      "10               0      0      0  \n",
      "11               0      0      0  \n",
      "14               0      0      0  \n",
      "\n",
      "[5 rows x 10503 columns]\n",
      "\n",
      "\n",
      "\n",
      "release_name  0      1      2      3      4      5      6      7      8      \\\n",
      "user_id                                                                       \n",
      "5                 0      0      0      0      0      0      0      0      0   \n",
      "6                 0      0      0      0      0      0      0      0      0   \n",
      "10                0      0      0      0      0      0      0      0      0   \n",
      "11                0      0      0      0      0      0      0      0      0   \n",
      "14                0      0      0      0      0      0      0      0      0   \n",
      "\n",
      "release_name  9      ...  12506  12507  12508  12509  12510  12511  12512  \\\n",
      "user_id              ...                                                    \n",
      "5                 0  ...      0      0      0      0      0      0      0   \n",
      "6                 0  ...      0      0      0      0      0      0      0   \n",
      "10                0  ...      0      0      0      0      0      0      0   \n",
      "11                0  ...      0      0      0      0      0      0      0   \n",
      "14                0  ...      0      0      0      0      0      0      0   \n",
      "\n",
      "release_name  12513  12514  12515  \n",
      "user_id                            \n",
      "5                 0      0      0  \n",
      "6                 0      0      0  \n",
      "10                0      0      0  \n",
      "11                0      0      0  \n",
      "14                0      0      0  \n",
      "\n",
      "[5 rows x 12516 columns]\n",
      "\n",
      "\n",
      "\n",
      "recording_name  0      1      2      3      4      5      6      7      8      \\\n",
      "user_id                                                                         \n",
      "5                   0      0      0      0      0      0      0      0      0   \n",
      "6                   0      0      0      0      0      0      0      0      0   \n",
      "10                  0      0      0      0      0      0      0      0      0   \n",
      "11                  0      0      0      0      0      0      0      0      0   \n",
      "14                  0      0      0      0      0      0      0      0      0   \n",
      "\n",
      "recording_name  9      ...  16141  16142  16143  16144  16145  16146  16147  \\\n",
      "user_id                ...                                                    \n",
      "5                   0  ...      0      0      0      0      0      0      0   \n",
      "6                   0  ...      0      0      0      0      0      0      0   \n",
      "10                  0  ...      0      0      0      0      0      0      0   \n",
      "11                  0  ...      0      0      0      0      0      0      0   \n",
      "14                  0  ...      0      0      0      0      0      0      0   \n",
      "\n",
      "recording_name  16148  16149  16150  \n",
      "user_id                              \n",
      "5                   0      0      0  \n",
      "6                   0      0      0  \n",
      "10                  0      0      0  \n",
      "11                  0      0      0  \n",
      "14                  0      0      0  \n",
      "\n",
      "[5 rows x 16151 columns]\n"
     ]
    }
   ],
   "source": [
    "#will make a series of matrix factorizations and store them with the appropriate names\n",
    "\n",
    "user_artist = df.pivot_table(index = 'user_id', columns = 'artist_name', aggfunc = 'size', fill_value = 0)\n",
    "user_release = df.pivot_table(index = 'user_id', columns = 'release_name', aggfunc = 'size', fill_value = 0)\n",
    "user_recording = df.pivot_table(index = 'user_id', columns = 'recording_name', aggfunc = 'size', fill_value = 0)\n",
    "\n",
    "print(user_artist.head())\n",
    "print(\"\\n\\n\")\n",
    "print(user_release.head())\n",
    "print(\"\\n\\n\")\n",
    "print(user_recording.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARQUET 0\n",
    "\n",
    "#import dataset as a dataframe from a csv\n",
    "df = pd.read_csv('../../Project1/completeData0.csv')\n",
    "\n",
    "df['artist_name'] = df['artist_name'].astype('category').cat.codes\n",
    "df['release_name'] = df['release_name'].astype('category').cat.codes\n",
    "df['recording_name'] = df['recording_name'].astype('category').cat.codes\n",
    "\n",
    "df2 = df.drop(['date', 'time'], axis = 1)\n",
    "\n",
    "#train/test split\n",
    "#80/20\n",
    "train, test = train_test_split(df2, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cuml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#utilizes GPU on PARQUET 0\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcuml\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m silhouette_score\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cuml'"
     ]
    }
   ],
   "source": [
    "# PARQUET 0\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Define a range of eps and minPts values to search over\n",
    "eps_range = [200000, 500000] #eps_range = [0.1, 0.5, 1.0, 1.5]\n",
    "minPts_range = [5, 10, 15, 20] #minPts_range = [5, 10, 15, 20]\n",
    "\n",
    "best_score = -1\n",
    "best_eps = None\n",
    "best_minPts = None\n",
    "\n",
    "for eps in eps_range:\n",
    "    for minPts in minPts_range:\n",
    "        print(f\"Current eps is: {eps} and current minPts is: {minPts}\")\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=minPts)\n",
    "        labels = dbscan.fit_predict(train)\n",
    "        silhouette = silhouette_score(train, labels)\n",
    "        if silhouette > best_score:\n",
    "            best_score = silhouette\n",
    "            best_eps = eps\n",
    "            best_minPts = minPts\n",
    "\n",
    "print(\"Best eps:\", best_eps)\n",
    "print(\"Best minPts:\", best_minPts)\n",
    "print(\"Best Score is:\", best_score)\n",
    "\n",
    "print(\"Previous bests were: eps = 2000, minPts = 5, score = 0.30843316542674687\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current eps is: 2000 and current minPts is: 5\n",
      "checking validity\n",
      "Current eps is: 2000 and current minPts is: 10\n",
      "checking validity\n",
      "Current eps is: 2000 and current minPts is: 15\n",
      "checking validity\n",
      "Current eps is: 2000 and current minPts is: 20\n",
      "checking validity\n",
      "Current eps is: 3000 and current minPts is: 5\n",
      "checking validity\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of labels is 1. Valid values are 2 to n_samples - 1 (inclusive)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(labels) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchecking validity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m     silhouette \u001b[38;5;241m=\u001b[39m \u001b[43msilhouette_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m silhouette \u001b[38;5;241m>\u001b[39m best_score:\n\u001b[0;32m     21\u001b[0m         best_score \u001b[38;5;241m=\u001b[39m silhouette\n",
      "File \u001b[1;32mc:\\Users\\derpi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\derpi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:140\u001b[0m, in \u001b[0;36msilhouette_score\u001b[1;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m         X, labels \u001b[38;5;241m=\u001b[39m X[indices], labels[indices]\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43msilhouette_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\derpi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\derpi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:297\u001b[0m, in \u001b[0;36msilhouette_samples\u001b[1;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[0;32m    295\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[0;32m    296\u001b[0m label_freqs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(labels)\n\u001b[1;32m--> 297\u001b[0m \u001b[43mcheck_number_of_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metric\n\u001b[0;32m    300\u001b[0m reduce_func \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[0;32m    301\u001b[0m     _silhouette_reduce, labels\u001b[38;5;241m=\u001b[39mlabels, label_freqs\u001b[38;5;241m=\u001b[39mlabel_freqs\n\u001b[0;32m    302\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\derpi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:37\u001b[0m, in \u001b[0;36mcheck_number_of_labels\u001b[1;34m(n_labels, n_samples)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that number of labels are valid.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m    Number of samples.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m n_labels \u001b[38;5;241m<\u001b[39m n_samples:\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of labels is \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. Valid values are 2 to n_samples - 1 (inclusive)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;241m%\u001b[39m n_labels\n\u001b[0;32m     40\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Number of labels is 1. Valid values are 2 to n_samples - 1 (inclusive)"
     ]
    }
   ],
   "source": [
    "# PARQUET 17\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Define a range of eps and minPts values to search over\n",
    "eps_range = [2000, 3000, 4000] #eps_range = [0.1, 0.5, 1.0, 1.5]\n",
    "minPts_range = [5, 10, 15, 20] #minPts_range = [5, 10, 15, 20]\n",
    "\n",
    "best_score = -1\n",
    "best_eps = None\n",
    "best_minPts = None\n",
    "\n",
    "for eps in eps_range:\n",
    "    for minPts in minPts_range:\n",
    "        print(f\"Current eps is: {eps} and current minPts is: {minPts}\")\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=minPts)\n",
    "        labels = dbscan.fit_predict(train)\n",
    "        silhouette = silhouette_score(train, labels)\n",
    "        if silhouette > best_score:\n",
    "            best_score = silhouette\n",
    "            best_eps = eps\n",
    "            best_minPts = minPts\n",
    "\n",
    "print(\"Best eps:\", best_eps)\n",
    "print(\"Best minPts:\", best_minPts)\n",
    "print(\"Best Score is:\", best_score)\n",
    "\n",
    "print(\"Previous bests were: eps = 2000, minPts = 5, score = 0.30843316542674687\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCF algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New attempt on user item interactions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\derpi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dot, Dense, Flatten, Dropout\n",
    "import numpy as np\n",
    "\n",
    "#flatten user item interaction matrix\n",
    "user_ids_flat = np.repeat(user_artist.index.to_numpy(), len(user_artist.columns.to_numpy()))\n",
    "item_ids_flat = np.tile(user_artist.columns.to_numpy(), len(user_artist.index.to_numpy()))\n",
    "\n",
    "# Flattened labels\n",
    "labels = user_artist.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\derpi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create NCF model structure\n",
    "\n",
    "#input layers for user and item\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "item_input = Input(shape=(1,), name='item_input')\n",
    "\n",
    "#embedding layers to flatten the information from user-item interaction matrix\n",
    "user_embedding = Embedding(input_dim=len(user_ids_flat), output_dim=16, name='user_embedding')(user_input)\n",
    "item_embedding = Embedding(input_dim=len(item_ids_flat), output_dim=16, name='item_embedding')(item_input)\n",
    "\n",
    "#flatten embedding layers for dot product\n",
    "user_vec = Flatten()(user_embedding)\n",
    "item_vec = Flatten()(item_embedding)\n",
    "\n",
    "#dot product between user and item vectors (matrices)\n",
    "dot_product = Dot(axes=1)([user_vec, item_vec])\n",
    "\n",
    "#add 1 hidden layer with 512 neurons\n",
    "x = Dense(512, activation='relu')(dot_product)\n",
    "\n",
    "#add 20% dropout to reduce overfitting\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "#output with sigmoid activation function for binary classification (user likes artist)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "#defining the model\n",
    "ncf_model = Model(inputs=[user_input, item_input], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\derpi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model compilation\n",
    "ncf_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "524/524 [==============================] - 1331s 3s/step - loss: 0.0479 - accuracy: 0.9982\n",
      "Epoch 2/10\n",
      "524/524 [==============================] - 1326s 3s/step - loss: 0.0347 - accuracy: 0.9982\n",
      "Epoch 3/10\n",
      "524/524 [==============================] - 1326s 3s/step - loss: 0.0347 - accuracy: 0.9982\n",
      "Epoch 4/10\n",
      "524/524 [==============================] - 1326s 3s/step - loss: 0.0347 - accuracy: 0.9982\n",
      "Epoch 5/10\n",
      "524/524 [==============================] - 1328s 3s/step - loss: 0.0347 - accuracy: 0.9982\n",
      "Epoch 6/10\n",
      "524/524 [==============================] - 1333s 3s/step - loss: 0.0347 - accuracy: 0.9982\n",
      "Epoch 7/10\n",
      "524/524 [==============================] - 1326s 3s/step - loss: 0.0340 - accuracy: 0.9981\n",
      "Epoch 8/10\n",
      "524/524 [==============================] - 1326s 3s/step - loss: 0.0331 - accuracy: 0.9980\n",
      "Epoch 9/10\n",
      "524/524 [==============================] - 1326s 3s/step - loss: 0.0324 - accuracy: 0.9980\n",
      "Epoch 10/10\n",
      "524/524 [==============================] - 1326s 3s/step - loss: 0.0320 - accuracy: 0.9980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17b6f00dbd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#insert data as a single table into the model\n",
    "user_item_data = [user_ids_flat, item_ids_flat]\n",
    "\n",
    "#train the model on the combined data\n",
    "ncf_model.fit(user_item_data, labels, epochs=10, batch_size=32768)\n",
    "\n",
    "#111 minutes for 5 epochs with batch size 32768 for binary cross entropy loss function\n",
    "\n",
    "#221 minutes for 10 epochs with batch size 32768 for MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "535982/535982 [==============================] - 416s 776us/step - loss: 0.0315 - accuracy: 0.9982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0314859077334404, 0.9981666803359985]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model eval\n",
    "ncf_model.evaluate(user_item_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  40  41  42  43  44  45  46  47  51  52  53  56  58  59  60\n",
      "  61  62  63  64  66  72  76  78  86  87  91  94  96  99 118 198]\n"
     ]
    }
   ],
   "source": [
    "#why is my loss negative?\n",
    "#does the data not have only binary values??\n",
    "\n",
    "#Binary Cross-Entropy Loss = âˆ’ (y_actual * log(y_pred) + (1 - y_actual) * log(1 - y_pred))\n",
    "\n",
    "print(np.unique(labels))\n",
    "\n",
    "#it does lol\n",
    "\n",
    "#will change the loss function from BCE to MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input layers\n",
    "user_id_input = Input(shape=(1,), name='user_id_input')\n",
    "artist_name_input = Input(shape=(1,), name='artist_name_input')\n",
    "release_name_input = Input(shape=(1,), name='release_name_input')\n",
    "recording_name_input = Input(shape=(1,), name='recording_name_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding size for embedding layers\n",
    "embeddingSize = 50\n",
    "\n",
    "#embedding layers\n",
    "user_embedding = Embedding(input_dim=len(df['user_id'].unique()), output_dim=embeddingSize)(user_id_input)\n",
    "artist_embedding = Embedding(input_dim=len(df['artist_name'].unique()), output_dim=embeddingSize)(artist_name_input)\n",
    "release_embedding = Embedding(input_dim=len(df['release_name'].unique()), output_dim=embeddingSize)(release_name_input)\n",
    "recording_embedding = Embedding(input_dim=len(df['recording_name'].unique()), output_dim=embeddingSize)(recording_name_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate embeddings then flatten them\n",
    "concatenated = Concatenate()([user_embedding, artist_embedding, release_embedding, recording_embedding])\n",
    "flatten = Flatten()(concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add dense layers for learning\n",
    "hidden1 = Dense(128, activation = 'relu')(flatten)\n",
    "hidden2 = Dense(64, activation = 'relu')(hidden1)\n",
    "output = Dense(1)(hidden2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "model = Model(inputs = [user_id_input, artist_name_input, release_name_input, recording_name_input], outputs = output)\n",
    "\n",
    "#compile model\n",
    "model.compile(optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\derpi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\derpi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\derpi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\derpi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1152, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"c:\\Users\\derpi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1106, in _validate_target_and_loss\n        raise ValueError(\n\n    ValueError: Target data is missing. Your model was compiled with loss=mse, and therefore expects target data to be provided in `fit()`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#train model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43martist_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelease_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecording_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\derpi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filet3u3cu9q.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\derpi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\derpi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\derpi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\derpi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1152, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"c:\\Users\\derpi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1106, in _validate_target_and_loss\n        raise ValueError(\n\n    ValueError: Target data is missing. Your model was compiled with loss=mse, and therefore expects target data to be provided in `fit()`.\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "model.fit([train['user_id'], train['artist_name'], train['release_name'], train['recording_name']], epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idk how to do it yet but i will figure it out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
